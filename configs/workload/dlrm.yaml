model: dlrm

framework: pytorch

# Need a num_gpus parameter since we run with a single data loader
num_gpus: 8

workflow:
  generate_data: False
  train: True
  profiling: False
  checkpoint: True
  evaluation: True

dataset: 
  data_folder: data/dlrm
  format: bin
  num_files_train: 1
  num_files_eval: 1
  num_samples_per_file: 4195198976
  record_length: 327680
  keep_files: True
  eval_num_samples_per_file: 91681240
  
reader: 
  data_loader: terabyte
  batch_size: 32768
  batch_size_eval: 16384

train:
  epochs: 1
  total_training_steps: 32768

evaluation: 
  eval_time: 0.0843
  total_eval_steps: 2048
  steps_between_evals: 16384

checkpoint:
  steps_between_checkpoints: 16384
  model_size: 27129166561
  ckpt_write_sz_q1: 7348
  ckpt_write_sz_mean: 517653535
  ckpt_write_sz_std: 841116648
  ckpt_write_sz_min: 98